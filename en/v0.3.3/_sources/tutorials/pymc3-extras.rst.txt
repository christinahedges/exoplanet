.. module:: exoplanet

.. note:: This tutorial was generated from an IPython notebook that can be
          downloaded `here <../../_static/notebooks/pymc3-extras.ipynb>`_.

.. _pymc3-extras:



PyMC3 extras
============

*exoplanet* comes bundled with a few utilities that can make it easier
to use and debug PyMC3 models for fitting exoplanet data. This tutorial
briefly describes these features and their use.

Dense mass matrices
-------------------

The main extra is the :func:`exoplanet.get_dense_nuts_step` function
that extends the PyMC3 sampling procedure to include support for
learning off-diagonal elements of the mass matrix. This is *very*
important for any problems where there are covariances between the
parameters (this is true for pretty much all exoplanet models). A
thorough discussion of this `can be found elsewhere
online <https://dfm.io/posts/pymc3-mass-matrix/>`__, but here is a
simple demo where we sample a covariant Gaussian using
:func:`exoplanet.get_dense_nuts_step`.

First, we generate a random positive definite covariance matrix for the
Gaussian:

.. code:: python

    import numpy as np
    
    ndim = 5
    np.random.seed(42)
    L = np.random.randn(ndim, ndim)
    L[np.diag_indices_from(L)] = 0.1 * np.exp(L[np.diag_indices_from(L)])
    L[np.triu_indices_from(L, 1)] = 0.0
    cov = np.dot(L, L.T)

And then we can sample this using PyMC3 and
:func:`exoplanet.get_dense_nuts_step`:

.. code:: python

    import pymc3 as pm
    import exoplanet as xo
    
    with pm.Model() as model:
        pm.MvNormal("x", mu=np.zeros(ndim), chol=L, shape=(ndim,))
        trace = pm.sample(
            tune=2000, draws=2000, chains=2, cores=2, step=xo.get_dense_nuts_step()
        )


.. parsed-literal::

    Multiprocess sampling (2 chains in 2 jobs)
    NUTS: [x]



.. raw:: html

    
    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [8000/8000 00:08<00:00 Sampling 2 chains, 0 divergences]
    </div>



.. parsed-literal::

    Sampling 2 chains for 2_000 tune and 2_000 draw iterations (4_000 + 4_000 draws total) took 9 seconds.


This is a little more verbose than the standard use of PyMC3, but the
performance is several orders of magnitude better than you would get
without the mass matrix tuning. As you can see from the
``pymc3.summary``, the autocorrelation time of this chain is about 1 as
we would expect for a simple problem like this.

.. code:: python

    pm.summary(trace)




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>mean</th>
          <th>sd</th>
          <th>hdi_3%</th>
          <th>hdi_97%</th>
          <th>mcse_mean</th>
          <th>mcse_sd</th>
          <th>ess_mean</th>
          <th>ess_sd</th>
          <th>ess_bulk</th>
          <th>ess_tail</th>
          <th>r_hat</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>x[0]</th>
          <td>0.001</td>
          <td>0.166</td>
          <td>-0.289</td>
          <td>0.325</td>
          <td>0.002</td>
          <td>0.003</td>
          <td>6970.0</td>
          <td>1688.0</td>
          <td>6997.0</td>
          <td>3068.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>x[1]</th>
          <td>-0.014</td>
          <td>0.529</td>
          <td>-0.993</td>
          <td>0.979</td>
          <td>0.006</td>
          <td>0.008</td>
          <td>7772.0</td>
          <td>1953.0</td>
          <td>7686.0</td>
          <td>3020.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>x[2]</th>
          <td>0.008</td>
          <td>0.665</td>
          <td>-1.243</td>
          <td>1.216</td>
          <td>0.008</td>
          <td>0.011</td>
          <td>7725.0</td>
          <td>1884.0</td>
          <td>7766.0</td>
          <td>3075.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>x[3]</th>
          <td>0.019</td>
          <td>1.182</td>
          <td>-2.315</td>
          <td>2.103</td>
          <td>0.014</td>
          <td>0.019</td>
          <td>7597.0</td>
          <td>1965.0</td>
          <td>7600.0</td>
          <td>3100.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>x[4]</th>
          <td>0.008</td>
          <td>2.061</td>
          <td>-3.881</td>
          <td>3.675</td>
          <td>0.025</td>
          <td>0.037</td>
          <td>6547.0</td>
          <td>1572.0</td>
          <td>6517.0</td>
          <td>3049.0</td>
          <td>1.0</td>
        </tr>
      </tbody>
    </table>
    </div>



Evaluating model components for specific samples
------------------------------------------------

I find that when I’m debugging a PyMC3 model, I often want to inspect
the value of some part of the model for a given set of parameters. As
far as I can tell, there isn’t a simple way to do this in PyMC3, so
*exoplanet* comes with a hack for doing this:
:func:`exoplanet.eval_in_model`. This function handles the mapping
between named PyMC3 variables and the input required by the Theano
function that can evaluate the requested variable or tensor.

As a demo, let’s say that we’re fitting a parabola to some data:

.. code:: python

    np.random.seed(42)
    x = np.sort(np.random.uniform(-1, 1, 50))
    with pm.Model() as model:
        logs = pm.Normal("logs", mu=-3.0, sd=1.0)
        a0 = pm.Normal("a0")
        a1 = pm.Normal("a1")
        a2 = pm.Normal("a2")
        mod = a0 + a1 * x + a2 * x ** 2
    
        # Sample from the prior
        prior_sample = pm.sample_prior_predictive(samples=1)
        y = xo.eval_in_model(mod, prior_sample)
        y += np.exp(prior_sample["logs"]) * np.random.randn(len(y))
    
        # Add the likelihood
        pm.Normal("obs", mu=mod, sd=pm.math.exp(logs), observed=y)
    
        # Fit the data
        map_soln = xo.optimize()
        trace = pm.sample(cores=2)


.. parsed-literal::

    optimizing logp for variables: [a2, a1, a0, logs]
    25it [00:00, 608.35it/s, logp=4.272312e+01]
    message: Optimization terminated successfully.
    logp: -180.51036900636572 -> 42.72311721910288
    Auto-assigning NUTS sampler...
    Initializing NUTS using jitter+adapt_diag...
    Multiprocess sampling (2 chains in 2 jobs)
    NUTS: [a2, a1, a0, logs]



.. raw:: html

    
    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [4000/4000 00:02<00:00 Sampling 2 chains, 0 divergences]
    </div>



.. parsed-literal::

    Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.


After running the fit, it might be interesting to look at the
predictions of the model. We could have added a ``pymc3.Deterministic``
node for eveything, but that can end up taking up a lot of memory and
sometimes its useful to be able to experiement with different outputs.
Using :func:`exoplanet.utils.eval_in_model` we can, for example,
evaluate the maximum a posteriori (MAP) model prediction on a fine grid:

.. code:: python

    import matplotlib.pyplot as plt
    
    x_grid = np.linspace(-1.1, 1.1, 5000)
    with model:
        pred = xo.eval_in_model(a0 + a1 * x_grid + a2 * x_grid ** 2, map_soln)
    
    plt.plot(x, y, ".k", label="data")
    plt.plot(x_grid, pred, label="map")
    plt.legend(fontsize=12)
    plt.xlabel("x")
    plt.ylabel("y")
    _ = plt.xlim(-1.1, 1.1)



.. image:: pymc3-extras_files/pymc3-extras_12_0.png


We can also combine this with :func:`exoplanet.get_samples_from_trace`
to plot this prediction for a set of samples in the trace.

.. code:: python

    samples = np.empty((50, len(x_grid)))
    with model:
        y_grid = a0 + a1 * x_grid + a2 * x_grid ** 2
        for i, sample in enumerate(xo.get_samples_from_trace(trace, size=50)):
            samples[i] = xo.eval_in_model(y_grid, sample)
    
    plt.plot(x, y, ".k", label="data")
    plt.plot(x_grid, pred, label="map")
    plt.plot(x_grid, samples[0], color="C1", alpha=0.1, label="posterior")
    plt.plot(x_grid, samples[1:].T, color="C1", alpha=0.1)
    plt.legend(fontsize=12)
    plt.xlabel("x")
    plt.ylabel("y")
    _ = plt.xlim(-1.1, 1.1)



.. image:: pymc3-extras_files/pymc3-extras_14_0.png


